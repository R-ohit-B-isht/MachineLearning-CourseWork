{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R-ohit-B-isht/MachineLearning-CourseWork/blob/main/Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGwkr4GESQ29"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"102053017_Assignment5.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1tZcWBOVl6yxNm72QyO-M_aruEIfZDQ6Z\n",
        "\n",
        "Q1.Multiclass Logistic Regression Implement Multiclass Logistic Regression (step-bystep) on Iris dataset using one vs. rest strategy?\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "class Classifier(object):\n",
        "    def _init_(self):\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, x, y, epochs=300, eta=0.01):\n",
        "        # self.w = np.random.normal(size=(x.shape[1], 1))\n",
        "        self.w = np.zeros(shape=(x.shape[1], 1))\n",
        "        self.b = 0\n",
        "        for _ in range(epochs):\n",
        "            for idx, ip in enumerate(x):\n",
        "                ip = ip.reshape(-1, 1)\n",
        "                target = y[idx]\n",
        "                pred = (self.w.T @ ip + self.b)\n",
        "                pred = pred.reshape(-1)\n",
        "                pred = pred[0]\n",
        "                pred = 1 if self.sigmoid(pred) > 0.5 else -1\n",
        "                gradient = pred - target\n",
        "                self.w -= eta * gradient * ip\n",
        "                self.b -= eta * gradient\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def predict(self, x):\n",
        "        pred_list = []\n",
        "        for ip in x:\n",
        "            ip = ip.reshape(-1)\n",
        "            pred = self.w.T @ ip + self.b\n",
        "            pred = pred.reshape(-1)\n",
        "            pred = pred[0]\n",
        "            pred = 1 if pred > 0 else -1\n",
        "            pred_list.append(pred)\n",
        "        return pred_list\n",
        "\n",
        "    iris_df = load_iris()\n",
        "    # print(iris_df[\"target_names\"])\n",
        "    X = iris_df[\"data\"]\n",
        "    y = iris_df[\"target\"]\n",
        "    # print(X.mean(axis=0), X.std(axis=0))\n",
        "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "    # class encoding\n",
        "    # 0 - setosa\n",
        "    # 1 - versicolor\n",
        "    # 2 - virginica\n",
        "    # first recongnise setosa vs non setosa\n",
        "    # second recognise versicolor vs non versicolor\n",
        "    # third recognise virginica vs non verginica\n",
        "    # clf_1 -> recognise only setosa and non setosa\n",
        "    clf_1 = Classifier()\n",
        "\n",
        "    clf_1.fit(X_train, np.where(y_train == 0, 1, -1))\n",
        "    y_pred = clf_1.predict(X_test)\n",
        "    ground_truth = np.where(y_test == 0, 1, -1)\n",
        "    print(\"Score 1: \", accuracy_score(ground_truth, y_pred))\n",
        "    # clf_2 -> recognise only versicolor and non versicolor\n",
        "    clf_2 = Classifier()\n",
        "    clf_2.fit(X_train, np.where(y_train == 1, 1, -1), epochs=400)\n",
        "    y_pred = clf_2.predict(X_test)\n",
        "    ground_truth = np.where(y_test == 1, 1, -1)\n",
        "    print(\"Score 2: \", accuracy_score(ground_truth, y_pred))\n",
        "\n",
        "    clf_3 = Classifier()\n",
        "    clf_3.fit(X_train, np.where(y_train == 2, 1, -1))\n",
        "    y_pred = clf_3.predict(X_test)\n",
        "    ground_truth = np.where(y_test == 2, 1, -1)\n",
        "    print(\"Score: 3\", accuracy_score(ground_truth, y_pred))\n",
        "    # Code the output (Binary Encoded Approach)\n",
        "    # Setosa = 1 -1 -1\n",
        "    # virginica = -1 1 -1\n",
        "    # versicolor = -1 -1 1\n",
        "    # rest of all cases are undefined\n",
        "    final_pred_list = []\n",
        "    pred_1_arr = clf_1.predict(X_test)\n",
        "    pred_2_arr = clf_2.predict(X_test)\n",
        "    pred_3_arr = clf_3.predict(X_test)\n",
        "\n",
        "    arr_len = len(pred_1_arr)\n",
        "    for idx in range(arr_len):\n",
        "        pred_1 = pred_1_arr[idx]\n",
        "        pred_2 = pred_2_arr[idx]\n",
        "        pred_3 = pred_3_arr[idx]\n",
        "        result = f\"{pred_1}{pred_2}{pred_3}\"\n",
        "        if result == \"1-1-1\":\n",
        "            final_pred_list.append(0)\n",
        "        elif result == \"-11-1\":\n",
        "            final_pred_list.append(1)\n",
        "        elif result == \"-1-11\":\n",
        "            final_pred_list.append(2)\n",
        "        else:\n",
        "            final_pred_list.append(np.random.choice([1, 2]))\n",
        "\n",
        "    print(\"Final Score: \", accuracy_score(y_test, final_pred_list))\n",
        "\n",
        "\"\"\"Q2.Ridge Logistic Regression Download the exam dataset from the following link:\n",
        "https://drive.google.com/file/d/1wH6ofvNGPmORFlCLt72WGhJYPZiXstYh/view\n",
        "?usp=sharing\n",
        "The dataset labels that whether or not the student will get admission on the basis of\n",
        "the two exam scores. The plot of the data against exam1 and exam2\n",
        "As clear from the figure, a linear decision boundary does not fit well. So, fit a Logistic\n",
        "Regression Classifier with polynomial function of test1 and test2 scores upto degree\n",
        "6 using\n",
        "i. Step-by-Step Logistic Regression (with no regularization; alpha=10; number\n",
        "of iterations=1000)\n",
        "ii. Step-by-Step Logistic Regression (with ridge regularization; alpha=10;\n",
        "number of iterations=1000; lambda=0.2)\n",
        "\"\"\"\n",
        "\n",
        "from sklearn import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "X,y = pd.read_csv(\"./dataset\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "poly = PolynomialFeatures(degree=6)\n",
        "X_trf = poly.fit_transform(X_train)\n",
        "X_tsf=poly.fit_transform(X_test)\n",
        "\n",
        "def initial_parameter(normalized_Data_part):\n",
        "    return(np.random.randn(1,1),np.random.randn(normalized_Data_part.shape[1],1))\n",
        "\n",
        "def sigmoid(normalized_Data_part,theta0_initial_val,theta_initial_val):\n",
        "    power=theta0_initial_val+np.dot(normalized_Data_part,theta_initial_val)\n",
        "    return(1/(1+ np.exp(-power)))\n",
        "\n",
        "def deri_wrt_theta0(temp,normalized_Data_part):\n",
        "    return((np.sum(temp))/normalized_Data_part.shape[0])\n",
        "\n",
        "def deri_wrt_theta(temp,normalized_Data_part,lam,theta_initial):\n",
        "    return(((np.matmul((normalized_Data_part.T),temp))/normalized_Data_part.shape[0]) -\n",
        "           ((lam/normalized_Data_part.shape[0])*theta_initial))\n",
        "    \n",
        "def cost_function(class_of_training_data,H_theta0_theta_A,theta_A,normalized_Data_part,lam):\n",
        "    a=np.matmul((class_of_training_data.T),np.log(H_theta0_theta_A))\n",
        "    b=np.matmul(((1-class_of_training_data).T),np.log(1-H_theta0_theta_A))\n",
        "    c=(a+b)/normalized_Data_part.shape[0]\n",
        "    d=(lam*np.sum(np.square(theta_A)))/2*normalized_Data_part.shape[0]\n",
        "    return(-c+d)\n",
        "\n",
        "def gradient_descent(alpha,epsilon,normalized_Data,class_of_training_data,lam):\n",
        "    theta0_initial,theta_initial=initial_parameter(normalized_Data)\n",
        "    i=0\n",
        "    iterations=[]\n",
        "    iter=1000\n",
        "    neg_log_like_loss = []\n",
        "    for i in range(iter):\n",
        "        H_theta0_theta_i=sigmoid(normalized_Data,theta0_initial,theta_initial)\n",
        "        Tem =H_theta0_theta_i-class_of_training_data\n",
        "        Derivative_theta0 =deri_wrt_theta0(Tem,normalized_Data)\n",
        "        Derivative_thetas =deri_wrt_theta(Tem,normalized_Data,lam,theta_initial)\n",
        "        neg_log_like_initial= cost_function(class_of_training_data,H_theta0_theta_i,theta_initial,normalized_Data,lam)\n",
        "        theta0_final = theta0_initial - alpha*Derivative_theta0\n",
        "        theta_final = theta_initial - alpha*Derivative_thetas\n",
        "        H_theta0_theta_f = sigmoid(normalized_Data,theta0_final,theta_final)\n",
        "        neg_log_like_final= cost_function(class_of_training_data,H_theta0_theta_f,theta_final,normalized_Data,lam)\n",
        "        if abs(neg_log_like_initial - neg_log_like_final) < epsilon:\n",
        "            return(theta0_final,theta_final,iterations,neg_log_like_loss)\n",
        "            break\n",
        "        theta0_initial = theta0_final\n",
        "        theta_initial = theta_final\n",
        "        iterations.append(i)\n",
        "        neg_log_like_loss.append(neg_log_like_initial)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}